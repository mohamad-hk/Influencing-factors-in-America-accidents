# پیش‌ بینی شدت تصادف با XGBoost

## معرفی پروژه

در این پروژه از دیتاست [US Accidents](https://www.kaggle.com/datasets/sobhanmoosavi/us-accidents) استفاده شده است. هدف اصلی، پیش‌بینی شدت تصادف و بررسی عواملی است که بیشترین تأثیر را بر شدت تصادف دارند. برای این منظوراز **XGBoost** استفاده شده است 

---

## پیش‌ نیازها
 
برای اجرای این پروژه نیاز به نصب پکیج‌ های زیر داریم:

```
pandas
cudf
numpy
matplotlib
seaborn
scikit-learn
xgboost
```

## مراحل انجام پروژه

# 1. خواندن داده‌ها
دیتاست اصلی بیش از 7 میلیون رکورد دارد. ابتدا داده‌ها را بارگذاری و سپس مراحل پیش پردازش را انجام می دهیم.

# 2. پیش‌ پردازش داده ‌ها

با توجه به این که حجم داده زیاد است، باید عملیات پیش پردازش بر روری داده ها که شامل چندین مرحله است را بر روی داده ها اعمال کنیم. در ادامه هر مرحله با جزیئات توضیح داده می شود.

## 2.1 حذف ستون های غیر ضروری
برای پیش‌ پردازش داده‌ ها، ستون‌ های غیر ضروری مانند ID، Description، Country، Source و Timezone حذف شدند، مقادیر گمشده با میانه یا صفر جایگزین شدند، داده‌ه ای متنی با استفاده از LabelEncoder به مقادیر عددی تبدیل شدند و در نهایت برای کاهش حجم داده، انواع داده‌ ها با کمک تابع auto_data_type به کوچک‌ ترین نوع ممکن تغییر یافتند.


## 2.2 پر کردن مقادیر خالی داده‌ها
در بعضی رکورد های داده یک سری ستون فیلد های خالی وجود داشت که بر اساس مقادیر آن ستون می تواند داشته باشد، آن مقادیر خالی را با بهترین مقادیر پر کردیم تا تحلیل مان درست باشد.در بعضی ستون ها مانند Wind_Speed,Pressure,Visibility,Humidity مقادیر را با میانه هر ستون پر کردیم. در ستون های End_Lat,End_Lng مقادیر شان را با به ترتیب با Start_Lat,Start_Lng پر کردیم زیرا این عمل باعث می شود توزیع داده هامون حفظ شود و تاثیر منفی ای بر روی تحلیل مان نگذارد.

## 2.3 تقسیم داده‌ها
بعضی ستون ها مانند City, State, Weather_Condition,Street, Wind_Direction, Sunrise_Sunset, Civil_Twilight, Nautical_Twilight, Astronomical_Twilight,County که مقادیر گسسته ای داشتند آن ها  را با LabelEncoder تبدیل به یک سری اعداد کردیم که حجم داده مان کاهش یابد


## 2.4 تبدیل داده های رشته ای
ستون ‌های (`Start_Time`, `End_Time`, `Weather_Timestamp`) به ویژگی‌ های عددی شامل سال، ماه، روز، ساعت و روز هفته تبدیل شدند، چون XGBoost با داده هایی که نوع آنها object است نمی تواند آموزش ببیند و ستون `Zipcode` نیز به صورت عددی پردازش شد .



# 3. تقسیم داده‌ها
داده‌ها به مجموعه‌های `train` و `test`  با نسبت 80 درصد برای آموزش و 20 درصد برای تست تقسیم شدند.


# 4. آموزش مد و ارزیابی نتایج
دو حالت متفاوت برای آموزش مدل XGBoost بررسی شد؛ یک ‌بار بدون وزن ‌دهی کلاس‌ ها و بار دیگر با اعمال وزن‌ دهی دستی. در حالت بدون وزن ‌دهی، مدل عملکرد بسیار خوبی بر روی کلاس اکثریت یعنی severity با کلاس 1 داشت و دقت کلی نسبتاً بالایی به دست آمد. با این حال، به دلیل نا متوازن بودن داده‌ ها، مدل در شناسایی کلاس‌ های اقلیت یعنی severity  های کلاس 3 و 0 عملکرد ضعیفی داشت. در مقابل، زمانی که وزن ‌دهی به کلاس ‌ها اعمال شد، مدل متعادل ‌تر عمل کرد؛ به‌ طوری ‌که توانایی آن در شناسایی کلاس ‌های اقلیت افزایش و مقدار recall برای این کلاس ‌ها بهبود پیدا کرد. البته این تغییر باعث کاهش نسبی در دقت کلی همراه بود، چون بخشی از نمونه‌ های کلاس اکثریت به اشتباه به اقلیت‌ ها نسبت داده شده بود و در نتیجه precision افت کرد. بطور کلی ، شناسایی صحیح همه severity ها و جلوگیری از نادیده‌ گرفته‌ شدن کلاس ‌های اقلیت، استفاده از وزن‌ دهی ضروری است. 

تغییر پارامتر های مدل مانند افزایش n_estimator ها تاثیر زیادی بر روی متریک های مدل نداشته است. از طرف دیگر تغییر وزن کلاس ها تاثیر بیشتری بر روی نتایج داشتند تا تغییر گارمتر های مدل. همچنین تغییر نرخ یادگیری تاثیری کمی بر روی نتایج می داشت و در عین حال مدت زمان یادگیری هم افزایش پیدا می کرد.

<div>
<img src="/images/metric-without-class-weight.png" width="45%" />
<img src="/images/chart-without-class-weight.png" width="45%" />
</div>

همان طور که در نمودار ها مشاهده می شود، مهم ترین عوامل موثر در پیش بینی شدت تصادف در هر دو حالت مصور شده است.

<div>
<img src="/images/metric-class-weight.png" width="45%" />
<img src="/images/chart-class-weight.png.png" width="45%" />

</div>

